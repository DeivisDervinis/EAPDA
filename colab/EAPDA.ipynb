{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "EAPDA.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeivisDervinis/EAPDA/blob/main/colab/EAPDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TX-hk1ZKy8-"
      },
      "source": [
        "# Step 1. Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krDhb498o2Jo",
        "outputId": "2e2e97b6-4d98-4a28-a08e-c1f708b59aed"
      },
      "source": [
        "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n",
        "!pip install PyAudio\n",
        "!pip install pydub"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libportaudio2 is already the newest version (19.6.0-1).\n",
            "libportaudiocpp0 is already the newest version (19.6.0-1).\n",
            "portaudio19-dev is already the newest version (19.6.0-1).\n",
            "libasound2-dev is already the newest version (1.1.3-5ubuntu0.5).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "Requirement already satisfied: PyAudio in /usr/local/lib/python3.7/dist-packages (0.2.11)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.7/dist-packages (0.25.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocXbPKOdK-yQ"
      },
      "source": [
        "# Step 2. Create models folder and download the latest emotion recognition model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bcc3P4b_q2Xe",
        "outputId": "2d02bf76-da8d-45ed-cd47-b5337a018d68"
      },
      "source": [
        "!mkdir models\n",
        "!wget http://github.com/DeivisDervinis/EAPDA/raw/main/colab/models/audio.hdf5 -O models/audio.hdf5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-08 22:26:03--  http://github.com/DeivisDervinis/EAPDA/raw/main/colab/models/audio.hdf5\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/DeivisDervinis/EAPDA/raw/main/colab/models/audio.hdf5 [following]\n",
            "--2021-03-08 22:26:03--  https://github.com/DeivisDervinis/EAPDA/raw/main/colab/models/audio.hdf5\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/DeivisDervinis/EAPDA/main/colab/models/audio.hdf5 [following]\n",
            "--2021-03-08 22:26:03--  https://raw.githubusercontent.com/DeivisDervinis/EAPDA/main/colab/models/audio.hdf5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5368560 (5.1M) [application/octet-stream]\n",
            "Saving to: ‘models/audio.hdf5’\n",
            "\n",
            "models/audio.hdf5   100%[===================>]   5.12M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-03-08 22:26:03 (40.8 MB/s) - ‘models/audio.hdf5’ saved [5368560/5368560]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QksFjwy8LHK9"
      },
      "source": [
        "# Step 3. Download audio files for Emotion Recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62VZGuVAqka0",
        "outputId": "70adf9c7-ca19-4000-f4b7-3f2ac851cfaf"
      },
      "source": [
        "!mkdir data\n",
        "!wget http://github.com/DeivisDervinis/EAPDA/raw/main/colab/data/joined_sound_1.wav -O data/joined_sound_1.wav\n",
        "!wget http://github.com/DeivisDervinis/EAPDA/raw/main/colab/data/joined_sound_2.wav -O data/joined_sound_2.wav\n",
        "!wget http://github.com/DeivisDervinis/EAPDA/raw/main/colab/data/joined_sound_3.wav -O data/joined_sound_3.wav\n",
        "!wget http://github.com/DeivisDervinis/EAPDA/raw/main/colab/data/joined_sound_4.wav -O data/joined_sound_4.wav\n",
        "!wget http://github.com/DeivisDervinis/EAPDA/raw/main/colab/data/joined_sound_5.wav -O data/joined_sound_5.wav\n",
        "!wget http://github.com/DeivisDervinis/EAPDA/raw/main/colab/data/joined_sound_6.wav -O data/joined_sound_6.wav"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "URL transformed to HTTPS due to an HSTS policy\n",
            "--2021-03-08 23:17:53--  https://github.com/DeivisDervinis/EAPDA/raw/main/colab/data/joined_sound_1.wav\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/DeivisDervinis/EAPDA/main/colab/data/joined_sound_1.wav [following]\n",
            "--2021-03-08 23:17:53--  https://raw.githubusercontent.com/DeivisDervinis/EAPDA/main/colab/data/joined_sound_1.wav\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1396444 (1.3M) [audio/wav]\n",
            "Saving to: ‘data/joined_sound_1.wav’\n",
            "\n",
            "data/joined_sound_1 100%[===================>]   1.33M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-03-08 23:17:53 (26.7 MB/s) - ‘data/joined_sound_1.wav’ saved [1396444/1396444]\n",
            "\n",
            "URL transformed to HTTPS due to an HSTS policy\n",
            "--2021-03-08 23:17:53--  https://github.com/DeivisDervinis/EAPDA/raw/main/colab/data/joined_sound_2.wav\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/DeivisDervinis/EAPDA/main/colab/data/joined_sound_2.wav [following]\n",
            "--2021-03-08 23:17:54--  https://raw.githubusercontent.com/DeivisDervinis/EAPDA/main/colab/data/joined_sound_2.wav\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1546970 (1.5M) [audio/wav]\n",
            "Saving to: ‘data/joined_sound_2.wav’\n",
            "\n",
            "data/joined_sound_2 100%[===================>]   1.47M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-03-08 23:17:54 (26.9 MB/s) - ‘data/joined_sound_2.wav’ saved [1546970/1546970]\n",
            "\n",
            "URL transformed to HTTPS due to an HSTS policy\n",
            "--2021-03-08 23:17:54--  https://github.com/DeivisDervinis/EAPDA/raw/main/colab/data/joined_sound_3.wav\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/DeivisDervinis/EAPDA/main/colab/data/joined_sound_3.wav [following]\n",
            "--2021-03-08 23:17:54--  https://raw.githubusercontent.com/DeivisDervinis/EAPDA/main/colab/data/joined_sound_3.wav\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1503310 (1.4M) [audio/wav]\n",
            "Saving to: ‘data/joined_sound_3.wav’\n",
            "\n",
            "data/joined_sound_3 100%[===================>]   1.43M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-03-08 23:17:55 (23.8 MB/s) - ‘data/joined_sound_3.wav’ saved [1503310/1503310]\n",
            "\n",
            "URL transformed to HTTPS due to an HSTS policy\n",
            "--2021-03-08 23:17:55--  https://github.com/DeivisDervinis/EAPDA/raw/main/colab/data/joined_sound_4.wav\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/DeivisDervinis/EAPDA/main/colab/data/joined_sound_4.wav [following]\n",
            "--2021-03-08 23:17:55--  https://raw.githubusercontent.com/DeivisDervinis/EAPDA/main/colab/data/joined_sound_4.wav\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2035790 (1.9M) [audio/wav]\n",
            "Saving to: ‘data/joined_sound_4.wav’\n",
            "\n",
            "data/joined_sound_4 100%[===================>]   1.94M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-03-08 23:17:56 (13.0 MB/s) - ‘data/joined_sound_4.wav’ saved [2035790/2035790]\n",
            "\n",
            "URL transformed to HTTPS due to an HSTS policy\n",
            "--2021-03-08 23:17:56--  https://github.com/DeivisDervinis/EAPDA/raw/main/colab/data/joined_sound_5.wav\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/DeivisDervinis/EAPDA/main/colab/data/joined_sound_5.wav [following]\n",
            "--2021-03-08 23:17:56--  https://raw.githubusercontent.com/DeivisDervinis/EAPDA/main/colab/data/joined_sound_5.wav\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2015310 (1.9M) [audio/wav]\n",
            "Saving to: ‘data/joined_sound_5.wav’\n",
            "\n",
            "data/joined_sound_5 100%[===================>]   1.92M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-03-08 23:17:57 (19.7 MB/s) - ‘data/joined_sound_5.wav’ saved [2015310/2015310]\n",
            "\n",
            "URL transformed to HTTPS due to an HSTS policy\n",
            "--2021-03-08 23:17:57--  https://github.com/DeivisDervinis/EAPDA/raw/main/colab/data/joined_sound_6.wav\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/DeivisDervinis/EAPDA/main/colab/data/joined_sound_6.wav [following]\n",
            "--2021-03-08 23:17:57--  https://raw.githubusercontent.com/DeivisDervinis/EAPDA/main/colab/data/joined_sound_6.wav\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1168358 (1.1M) [audio/wav]\n",
            "Saving to: ‘data/joined_sound_6.wav’\n",
            "\n",
            "data/joined_sound_6 100%[===================>]   1.11M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-03-08 23:17:57 (17.2 MB/s) - ‘data/joined_sound_6.wav’ saved [1168358/1168358]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Kuqv-inYtx"
      },
      "source": [
        "## Basics ##\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "## Audio Preprocessing ##\n",
        "import pyaudio\n",
        "import wave\n",
        "import librosa\n",
        "from scipy.stats import zscore\n",
        "\n",
        "## Time Distributed CNN ##\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, TimeDistributed\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten\n",
        "from tensorflow.keras.layers import LSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QA-VJtVnYt2"
      },
      "source": [
        "class speechEmotionRecognition:\n",
        "\n",
        "    ###\n",
        "    ##Voice recording function\n",
        "    ##\n",
        "\n",
        "    def __init__(self, subdir_model=None):\n",
        "\n",
        "        # Load prediction model\n",
        "        if subdir_model is not None:\n",
        "            self._model = self.build_model()\n",
        "            self._model.load_weights(subdir_model)\n",
        "\n",
        "        # Emotion encoding\n",
        "        self._emotion = {0:'Angry', 1:'Disgust', 2:'Fear', 3:'Happy', 4:'Neutral', 5:'Sad', 6:'Surprise'}\n",
        "\n",
        "\n",
        "    ### Computing Mel-Spectrogram\n",
        "    def mel_spectrogram(self, y, sr=16000, n_fft=512, win_length=256, hop_length=128, window='hamming', n_mels=128, fmax=4000):\n",
        "\n",
        "        # Compute spectogram\n",
        "        mel_spect = np.abs(librosa.stft(y, n_fft=n_fft, window=window, win_length=win_length, hop_length=hop_length)) ** 2\n",
        "\n",
        "        # Compute mel spectrogram\n",
        "        mel_spect = librosa.feature.melspectrogram(S=mel_spect, sr=sr, n_mels=n_mels, fmax=fmax)\n",
        "\n",
        "        # Compute log-mel spectrogram\n",
        "        mel_spect = librosa.power_to_db(mel_spect, ref=np.max)\n",
        "\n",
        "        return np.asarray(mel_spect)\n",
        "\n",
        "\n",
        "\n",
        "    # Audio framing\n",
        "    def frame(self, y, win_step=64, win_size=128):\n",
        "\n",
        "        # Number of frames\n",
        "        nb_frames = 1 + int((y.shape[2] - win_size) / win_step)\n",
        "\n",
        "        # Framming\n",
        "        frames = np.zeros((y.shape[0], nb_frames, y.shape[1], win_size)).astype(np.float16)\n",
        "        for t in range(nb_frames):\n",
        "            frames[:,t,:,:] = np.copy(y[:,:,(t * win_step):(t * win_step + win_size)]).astype(np.float16)\n",
        "\n",
        "        return frames\n",
        "\n",
        "\n",
        "    # Builds TDCNN\n",
        "    def build_model(self):\n",
        "\n",
        "        # Clear Keras session\n",
        "        K.clear_session()\n",
        "\n",
        "        # Define input\n",
        "        input_y = Input(shape=(3, 128, 128, 1), name='Input_MELSPECT')\n",
        "\n",
        "        # First LFLB (local feature learning block)\n",
        "        y = TimeDistributed(Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='same'), name='Conv_1_MELSPECT')(input_y)\n",
        "        y = TimeDistributed(BatchNormalization(), name='BatchNorm_1_MELSPECT')(y)\n",
        "        y = TimeDistributed(Activation('elu'), name='Activ_1_MELSPECT')(y)\n",
        "        y = TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'), name='MaxPool_1_MELSPECT')(y)\n",
        "        y = TimeDistributed(Dropout(0.2), name='Drop_1_MELSPECT')(y)\n",
        "\n",
        "        # Second LFLB (local feature learning block)\n",
        "        y = TimeDistributed(Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='same'), name='Conv_2_MELSPECT')(y)\n",
        "        y = TimeDistributed(BatchNormalization(), name='BatchNorm_2_MELSPECT')(y)\n",
        "        y = TimeDistributed(Activation('elu'), name='Activ_2_MELSPECT')(y)\n",
        "        y = TimeDistributed(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding='same'), name='MaxPool_2_MELSPECT')(y)\n",
        "        y = TimeDistributed(Dropout(0.2), name='Drop_2_MELSPECT')(y)\n",
        "\n",
        "        # Third LFLB (local feature learning block)\n",
        "        y = TimeDistributed(Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='same'), name='Conv_3_MELSPECT')(y)\n",
        "        y = TimeDistributed(BatchNormalization(), name='BatchNorm_3_MELSPECT')(y)\n",
        "        y = TimeDistributed(Activation('elu'), name='Activ_3_MELSPECT')(y)\n",
        "        y = TimeDistributed(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding='same'), name='MaxPool_3_MELSPECT')(y)\n",
        "        y = TimeDistributed(Dropout(0.2), name='Drop_3_MELSPECT')(y)\n",
        "\n",
        "        # Fourth LFLB (local feature learning block)\n",
        "        y = TimeDistributed(Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding='same'), name='Conv_4_MELSPECT')(y)\n",
        "        y = TimeDistributed(BatchNormalization(), name='BatchNorm_4_MELSPECT')(y)\n",
        "        y = TimeDistributed(Activation('elu'), name='Activ_4_MELSPECT')(y)\n",
        "        y = TimeDistributed(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding='same'), name='MaxPool_4_MELSPECT')(y)\n",
        "        y = TimeDistributed(Dropout(0.2), name='Drop_4_MELSPECT')(y)\n",
        "\n",
        "        # Flat\n",
        "        y = TimeDistributed(Flatten(), name='Flat_MELSPECT')(y)\n",
        "\n",
        "        # LSTM layer\n",
        "        y = LSTM(256, return_sequences=False, dropout=0.2, name='LSTM_1')(y)\n",
        "\n",
        "        # Fully connected\n",
        "        y = Dense(7, activation='softmax', name='FC')(y)\n",
        "\n",
        "        # Build final model\n",
        "        model = Model(inputs=input_y, outputs=y)\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "    # Predict speech emotion over time\n",
        "    def predict_emotion_from_file(self, filename, chunk_step=16000, chunk_size=40000, predict_proba=False, sample_rate=16000):\n",
        "\n",
        "        # Read audio file\n",
        "        y, sr = librosa.core.load(filename, sr=sample_rate, offset=0.5)\n",
        "\n",
        "        # Split audio signals into chunks\n",
        "        chunks = self.frame(y.reshape(1, 1, -1), chunk_step, chunk_size)\n",
        "\n",
        "        # Reshape chunks\n",
        "        chunks = chunks.reshape(chunks.shape[1],chunks.shape[-1])\n",
        "\n",
        "        # Z-normalization\n",
        "        y = np.asarray(list(map(zscore, chunks)))\n",
        "\n",
        "        # Compute mel spectrogram\n",
        "        mel_spect = np.asarray(list(map(self.mel_spectrogram, y)))\n",
        "\n",
        "        # Time distributed Framing\n",
        "        mel_spect_ts = self.frame(mel_spect)\n",
        "\n",
        "        # Build X for time distributed CNN\n",
        "        X = mel_spect_ts.reshape(mel_spect_ts.shape[0],\n",
        "                                    mel_spect_ts.shape[1],\n",
        "                                    mel_spect_ts.shape[2],\n",
        "                                    mel_spect_ts.shape[3],\n",
        "                                    1)\n",
        "\n",
        "        # Predict emotion\n",
        "        if predict_proba is True:\n",
        "            predict = self._model.predict(X)\n",
        "        else:\n",
        "            predict = np.argmax(self._model.predict(X), axis=1)\n",
        "            predict = [self._emotion.get(emotion) for emotion in predict]\n",
        "\n",
        "        # Clear Keras session\n",
        "        K.clear_session()\n",
        "\n",
        "        # Predict timestamp\n",
        "        timestamp = np.concatenate([[chunk_size], np.ones((len(predict) - 1)) * chunk_step]).cumsum()\n",
        "        timestamp = np.round(timestamp / sample_rate)\n",
        "\n",
        "        return [predict, timestamp]\n",
        "\n",
        "\n",
        "\n",
        "    #Export emotions\n",
        "    def prediction_to_csv(self, predictions, filename, mode='w'):\n",
        "\n",
        "        # Write emotion in filename\n",
        "        with open(filename, mode) as f:\n",
        "            if mode == 'w':\n",
        "                f.write(\"EMOTIONS\"+'\\n')\n",
        "            for emotion in predictions:\n",
        "                f.write(str(emotion)+'\\n')\n",
        "            f.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v204cIUxnYt9"
      },
      "source": [
        "from pydub import AudioSegment\n",
        "from pydub.playback import play\n",
        "import math\n",
        "\n",
        "# Initiate SER object\n",
        "SER = speechEmotionRecognition()\n",
        "\n",
        "# Set the name of the audio file\n",
        "audio_file_name = \"data/joined_sound_3.wav\"\n",
        "\n",
        "# Sets the duration of audio\n",
        "audio_duration = math.floor(librosa.get_duration(filename=audio_file_name))\n",
        "# Starting slice and end slice values\n",
        "n = 0\n",
        "nSpan = 3\n",
        "\n",
        "# List of sliced segments\n",
        "listOfSegNames = []\n",
        "\n",
        "for i in range(0, (audio_duration + 1) - nSpan):   \n",
        "    t1 = n * 1000\n",
        "    t2 = (n + nSpan) * 1000\n",
        "    \n",
        "    oldAudio = AudioSegment.from_wav(audio_file_name)\n",
        "    newAudio = oldAudio[t1:t2]\n",
        "    seg_name = \"data/seg_\" + str(n) +\".wav\"\n",
        "    listOfSegNames.append(seg_name)\n",
        "    \n",
        "    #listOfSegments.append(newAudio)\n",
        "    newAudio.export(seg_name, format=\"wav\")    \n",
        "    n += 1\n",
        "\n",
        "#play(listOfSegments[5])\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtOAJ1TpnYt9"
      },
      "source": [
        "|from collections import Counter\n",
        "\n",
        "# Function that finds the most common occuring emotion\n",
        "def find_state_emotion(listOfEmotions):\n",
        "        \n",
        "        count = Counter(listOfEmotions)\n",
        "        max_val = max(count.values())\n",
        "        \n",
        "        return sorted(key for key, value in count.items() if value == max_val)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "772xGGuWnYt-",
        "outputId": "647df675-7d8c-48f7-aff3-4b3b20f73d3b"
      },
      "source": [
        "model_sub_dir = os.path.join('models', 'audio.hdf5')\n",
        "\n",
        "SER = speechEmotionRecognition(model_sub_dir)\n",
        "\n",
        "segmentEmoList = []\n",
        "\n",
        "emotionFoundFlag = False\n",
        "finalEmotion = \"\"\n",
        "\n",
        "\n",
        "# Run prediction for each segment\n",
        "for k in range(0, len(listOfSegNames)):\n",
        "    rec_sub_dir = listOfSegNames[k]\n",
        "    \n",
        "    # Predict emotion in voice at each time step\n",
        "    step = 1 # in sec\n",
        "    sample_rate = 16000 # in kHz\n",
        "    emotions, timestamp = SER.predict_emotion_from_file(rec_sub_dir, chunk_step=step*sample_rate)\n",
        "    \n",
        "    major_emotion = max(set(emotions), key=emotions.count)\n",
        "    segmentEmoList.append(major_emotion)\n",
        "    \n",
        "    \n",
        "    finalEmotion = find_state_emotion(segmentEmoList)\n",
        "    if len(finalEmotion) == 1:\n",
        "        emotionFoundFlag = True\n",
        "        break\n",
        "            \n",
        "\n",
        "\n",
        "if emotionFoundFlag == True:\n",
        "    print(\"Emotion of the voice recording: \"+str(finalEmotion))\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fcba63455f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Emotion of the voice recording: ['Disgust']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdawGaeOD6QM",
        "outputId": "e17babce-149d-411f-e6c7-8eac58a0bb1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Remove locally created files\r\n",
        "for segment in listOfSegNames:\r\n",
        "  !rm $segment\r\n",
        "  print(\"Removed: \"+ str(segment))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Removed: data/seg_0.wav\n",
            "Removed: data/seg_1.wav\n",
            "Removed: data/seg_2.wav\n",
            "Removed: data/seg_3.wav\n",
            "Removed: data/seg_4.wav\n",
            "Removed: data/seg_5.wav\n",
            "Removed: data/seg_6.wav\n",
            "Removed: data/seg_7.wav\n",
            "Removed: data/seg_8.wav\n",
            "Removed: data/seg_9.wav\n",
            "Removed: data/seg_10.wav\n",
            "Removed: data/seg_11.wav\n",
            "Removed: data/seg_12.wav\n",
            "Removed: data/seg_13.wav\n",
            "Removed: data/seg_14.wav\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K4KlM22JREz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}